{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfef1ade",
   "metadata": {},
   "source": [
    "#   NLP Text Classification\n",
    "\n",
    "## Objective:\n",
    "To develop machine learning models to classify emotions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e01405",
   "metadata": {},
   "source": [
    "# 1. Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231781cf",
   "metadata": {},
   "source": [
    "### 1.1: Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "816e0221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Comment Emotion\n",
      "0  i seriously hate one subject to death but now ...    fear\n",
      "1                 im so full of life i feel appalled   anger\n",
      "2  i sit here to write i start to dig out my feel...    fear\n",
      "3  ive been really angry with r and i feel like a...     joy\n",
      "4  i feel suspicious if there is no one outside l...    fear\n",
      "Index(['Comment', 'Emotion'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\asus\\Downloads\\nlp_dataset.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows and the columns of the dataframe\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7778458",
   "metadata": {},
   "source": [
    "### 1.2: Preprocess the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4228ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Comment  \\\n",
      "0  i seriously hate one subject to death but now ...   \n",
      "1                 im so full of life i feel appalled   \n",
      "2  i sit here to write i start to dig out my feel...   \n",
      "3  ive been really angry with r and i feel like a...   \n",
      "4  i feel suspicious if there is no one outside l...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  seriously hate one subject death feel reluctan...  \n",
      "1                         im full life feel appalled  \n",
      "2  sit write start dig feelings think afraid acce...  \n",
      "3  ive really angry r feel like idiot trusting fi...  \n",
      "4  feel suspicious one outside like rapture happe...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Replace 'Comment' with the actual column name\n",
    "df['cleaned_text'] = df['Comment'].apply(preprocess_text)\n",
    "\n",
    "# Display the cleaned text\n",
    "print(df[['Comment', 'cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d6726",
   "metadata": {},
   "source": [
    "### Explanation of Preprocessing\n",
    "\n",
    "* Lowercasing: Converting the text to lowercase helps maintain consistency (e.g., \"Happy\" and \"happy\" should be treated the same).\n",
    "\n",
    "* Punctuation removal: We remove punctuation as it generally doesn't add much value for text classification, but removing it ensures uniformity.\n",
    "\n",
    "* Number removal: Since numbers often don't hold significance in text classification (unless the context requires it), they are removed.\n",
    "\n",
    "* Tokenization: Breaking text into individual words allows us to analyze each word separately.\n",
    "\n",
    "* Stopwords removal: Words like \"the\", \"is\", \"at\", and \"and\" are common but don't contribute much to the classification task, so removing them focuses on the more meaningful content.\n",
    "\n",
    "### Impact on Model Performance\n",
    "\n",
    "Cleaning the text helps the model focus on the most relevant information. By removing noise (e.g., stopwords, punctuation, numbers), the model can more effectively learn the patterns in the data, improving its ability to classify emotions. Tokenization allows us to turn the text into units (words) that the model can process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5335919",
   "metadata": {},
   "source": [
    "# 2. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd6a9251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer shape: (5937, 8815)\n",
      "TfidfVectorizer shape: (5937, 8815)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Initialize CountVectorizer and TfidfVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the cleaned text into numerical features\n",
    "X_count = count_vectorizer.fit_transform(df['cleaned_text'])\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
    "\n",
    "# Check the shape of transformed data (rows: samples, columns: unique words/features)\n",
    "print(\"CountVectorizer shape:\", X_count.shape)\n",
    "print(\"TfidfVectorizer shape:\", X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c0198",
   "metadata": {},
   "source": [
    "### Explanation of Feature Extraction\n",
    "\n",
    "#### CountVectorizer\n",
    "\n",
    "* It creates a bag-of-words representation of the text. Each unique word in the text is assigned a feature index, and the value of that feature is the frequency of the word's occurrence in the text.\n",
    "* Example: If the text is \"I am happy\", the vector might look like [1, 1, 1] where each index corresponds to the word frequency.\n",
    "\n",
    "#### TfidfVectorizer:\n",
    "\n",
    "* In addition to word counts, the TF-IDF (Term Frequency-Inverse Document Frequency) method considers how important a word is by reducing the weight of commonly used words that appear in many documents.\n",
    "* It balances the frequency of a word with its informativeness. Frequent but less informative words (like \"the\", \"is\") are given lower weights.\n",
    "\n",
    "### Impact of Feature Extraction on Performance\n",
    "\n",
    "* CountVectorizer captures the raw frequency of words, which may work well but could give too much importance to common words.\n",
    "* TfidfVectorizer provides a more balanced view by lowering the importance of frequently occurring words, which helps prevent overfitting and improves generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154cc5d0",
   "metadata": {},
   "source": [
    "# 3. Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fc73f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['Emotion'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "nb_model = MultinomialNB()\n",
    "svm_model = SVC(kernel='linear')\n",
    "\n",
    "# Train Naive Bayes model\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Train SVM model\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d639945a",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "**Naive Bayes:** The Multinomial Naive Bayes model is well-suited for text data where the features are word counts or TF-IDF values. It works on the assumption of conditional independence between words given the class.\n",
    "\n",
    "**SVM:** The Support Vector Machine (SVM) model tries to find a hyperplane that best separates the data into different classes. For text classification, a linear kernel is often effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e09342",
   "metadata": {},
   "source": [
    "# 4. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "899fc3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.9116161616161617\n",
      "Naive Bayes Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.88      0.95      0.91       392\n",
      "        fear       0.92      0.92      0.92       416\n",
      "         joy       0.95      0.87      0.90       380\n",
      "\n",
      "    accuracy                           0.91      1188\n",
      "   macro avg       0.91      0.91      0.91      1188\n",
      "weighted avg       0.91      0.91      0.91      1188\n",
      "\n",
      "SVM Accuracy: 0.946969696969697\n",
      "SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.93      0.96      0.94       392\n",
      "        fear       0.97      0.91      0.94       416\n",
      "         joy       0.94      0.97      0.96       380\n",
      "\n",
      "    accuracy                           0.95      1188\n",
      "   macro avg       0.95      0.95      0.95      1188\n",
      "weighted avg       0.95      0.95      0.95      1188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions for Naive Bayes\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "\n",
    "# Predictions for SVM\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "# Accuracy and Classification Report for Naive Bayes\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"Naive Bayes Classification Report:\\n\", classification_report(y_test, y_pred_nb))\n",
    "\n",
    "# Accuracy and Classification Report for SVM\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"SVM Classification Report:\\n\", classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03048f54",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "**Accuracy:** Measures the percentage of correct predictions. While useful, accuracy alone can be misleading if the dataset is imbalanced.\n",
    "\n",
    "**F1-score:** A balance between precision and recall, the F1-score is a good measure when classes are imbalanced (i.e., when some emotions may occur more often than others).\n",
    "\n",
    "**Naive Bayes:** It's computationally efficient and works well when the assumptions of word independence hold, which often works surprisingly well in text classification tasks.\n",
    "\n",
    "**SVM:** SVM is often more accurate as it finds the optimal boundary between classes. However, it is computationally more expensive than Naive Bayes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
